## consumer

``` java
Properties prop = new Properties();
prop.put("bootstrap.servers","localhost:9092")
prop.put("group.id","group1"); // group.id 지정
prop.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
prop.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

// Properties를 이용해서 KafkaConsumer 객체를 생성 함
KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(prop);
consumer.subscribe(Collections.singleton("simple")); // 토픽 구독
while(조건) {
  // 일정 시간 대기하면서 브로커로부터 컨슈머 레코드 목록을 읽어 온다
  ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
  // 읽어온 컨슈머 레코드 목록을 순회하며 필요한 처리를 진행
  for (ConsumerRecord<String, String> records : records) {
    System.out.println(record.value() + ":" + record.topic() + ":" + record.partition() + ":" + record.offset());
  }
}
// 사용 완료 후 close 메서드로 닫아줌
consumer.close();
```
- 토픽 파티션에서 레코드 조회
- subscribe 메서드 호출 시 내가 구독할 토픽 호출.

### 토픽 파티션은 그룹 단위 할당
![](https://velog.velcdn.com/images/gon109/post/dee4ba31-8787-4e9d-a922-08a58362b3ae/image.png)
- 파티션 갯수 보다 컨슈머가 많아지면 컨슈머는 놀게된다.
- 컨슈머 갯수가 파티션 갯수보다 커지면 안됨. 만약 처리량이 떨어져서 컨슈머를 늘려야된다면 파티션 갯수도 늘려야 한다.

### 커밋과 오프셋
![](https://velog.velcdn.com/images/gon109/post/40ec7cc1-3a65-4023-8b2e-26ba3b5b7ec3/image.png)


#### 커밋된 오프셋이 없는 경우
- 처음접근이거나 커밋 오프셋이 없는 경우
#### auto.offset.reset 설정 사용
- ealiest : 맨 처음 오프셋 사용
- latest: 가장 마지막 오프셋 사용(기본값)
- none: 컨슈머 그룹에 대한 이전 커밋이 없으면 익셉션 발생


### 컨슈머 설정

1. fetch.min.bytes: 조회시 브로커가 전송할 최소 데이커 크기
- 기본값: 1
- 이 값이 크면 대기 시간은 늘지만 처리량이 증가

2. fetch.max.wait.ms: 데이터가 최소 크기가 될 때까지 기다릴 시간
- 기본값: 500
- 브로커가 리턴할 때까지 대기하는 시간으로 poll() 메서드의 대기 시간과 다름

3. max.partition.fetch.bytes: 파티션 당 서버가 리턴할 수 있는 최대 크기
- 기본값 1MB


### 자동 커밋/ 수동 커밋

1. eanble.auto.commit
- true: 일정 주기로 컨슈머가 읽은 오프셋을 커밋 (기본값)
- false: 수동으로 커밋 실행
2. auto.commit.intervals.ms: 자동 커밋 주기
- 기본값: 5000(5초)

- pull(), close() 메서드 호출 시 자동 커밋 실행

### 수동 커밋: 동기/ 비동기 커밋

#### 동기 코드
``` java
ConsumerRecords<String, String> records = consumer.poll(Duration.ofSecond(1));
for (ConsumerRecord<String, String> record : records) {
  ... 처리
}
try {
  consumer.commitSync();
} catch(Exception ex) {
  // 커밋 실패시 에러 발생
}
```
- 커밋에 성공하면 익셉션 발생 x, 실패시 익셉션 발생

#### 비동기 코드
``` java
ConsumerRecords<String, String> records = consumer.poll(Duration.ofSecond(1));
for (ConsumerRecord<String, String> record : records) {
  ... 처리
}
consumer.commitAsync(); // commitAsync(OffsetCommitCallback callback)
```
- 비동기기때문에 코드 자체에서 실패여부를 알 수 없음
- 성공/ 실패 여부를 알고 싶다면 콜백을 받아서 처리해야 함


### 재처리와 순서

동일 메시지 조회 가능성
- 일시적 커밋 실패, 리밸런스 등에 의해 발생

컨슈머는 멱등성을 고려해야 함
- 조회수 1증가-> 좋아요 1증가-> 조회수 1증가
- 단순 처리하면 조회수는 2가 아닌 4가 될 수 있음
데이터 특성에 따라 타임스탬프, 일련 번호 등을 활용


### 세션 타임아웃, 하트비트, 최대 poll 간격
- 카프카는 컨슈머 그룹을 알맞게 유지하기 위해 몇가지 설정을 사용

컨슈머는 하트비트를 전송해서 연결 유지
- 브로커는 일정 시간동안 컨슈머로부터 하트비트가 없으면 컨슈머를 그룹에서 빼고 리밸런스 진행

1. session.timeout.ms: 세션 타임 아웃 시간 (기본값 10초) : 지정한 시간동안 하트비트가 없으면 컨슈머 빼버리는 시간
2. heartbeat.interval.ms: 하트비트 전송 주기 (기본값 3초) : 하트비트 전송 주기, session.timeout.ms의 1/3 이하 추천

3. max.poll.interval.ms: poll() 메서드의 최대 호출 간격 : 이 시간이 지나도록 poll()하지 않으면 컨슈머를 그룹에서 빼고 리밸런싱 진행

### 종료 처리
``` java
KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(prop);
consumer.subscribe(Collections.singleton("simple"));
try {
  while(조건) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100)); // wakeup() 호출시 Exception 발생
    ... records 처리
    try {
      consumer.commitSync();
    } catch(Exception ex) {
      e.printStackTrace();
    }
  }
} catch (Exception ex) {
  ...
} finally {
  consumer.close();
}
```
- 컨슈머를 다 사용하고 나면 종료해야함
- while 루프에서 벗어나려면, 다른 스레드에서 wakeup 호출 시 익셉션 발생하여 종료 처리

- KafkaConsumer는 쓰레드에 안전하지 않음
- 여러 쓰레드에서 동시에 사용하지 말 것. wakeup() 메서드는 예외







